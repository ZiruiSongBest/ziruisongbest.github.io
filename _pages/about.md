---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

[//]: # ()
[//]: # ()
Hi üëã, I'm Zirui Song(/ÀàziÀêÀåruÀê.i/ /s…îÀê≈ã/), a forth year Undergraduate at University of Technology Sydeny(UTS) majoring in Software Engineering. Also I am a visiting student supervised by <a href="https://scholar.google.com.au/citations?hl=en&user=COUnAF4AAAAJ&view_op=list_works&sortby=pubdate">Prof. Xiuying Chen</a> in <a href="https://www.mbzuai.ac.ae">MBZUAI</a> NLP department . I have become a member of <a href="https://utsnlp.github.io/">UTS-NLP</a> since Oct 2023 where I am fortuante to be advised by  <a href="https://profiles.uts.edu.au/Ling.Chen">Prof. Ling Chen</a>, and be mentored by <a href="https://mengfn.github.io/">Prof. Meng Fang</a>.
I am deeply appreciative of my mentor, <a href="https://dayan-guan.github.io/">Prof. Dayan Guan</a>, who guided me into scientific research.








>  I am very excited to discuss potential collaborations, please feel free to [contact me](ziruisong2003@gmail.com).

# üíª News

<p style="color: purple; font-weight: bold;">
2025-03-01: ‚ÄúI was admitted to the PhD program in the NLP department at <a href=""> MBZUAI</a>, where I will commence my PhD studies in August 2025 here.‚Äù</p>
<p style="color: red; font-weight: bold;">
2025-01-23: <a href="">One paper</a>  was accepted by NAACL 2025. </p>

<p style="color: red; font-weight: bold;">
2025-01-02: <a href="">One paper</a>  was accepted by Communications Chemistry. </p>

2024-09-25: First day as a visiting student at MBZUAI under the supervision of Prof. Xiuying Chen. <br>

<p style="color: red; font-weight: bold;">
2024-09-20: <a href="">One paper</a>  was accepted by EMNLP 2024. </p>
<p style="color: red; font-weight: bold;">
2024-07-01: <a href="">One paper</a>  was accepted by ECCV 2024. </p> 
	
2023-11-29: Prof. Ling Chen had accepted me as an undergraduate research assistant at Australia Artificial Intelligence Institute(AAII). <br>
2023-07-01: I am honored to be selected as an international exchange student majoring in Softawre Engineering at UTS. <br>
2023-05-18: Prof. Dayan Guan had accepted me as a remote undergraduate research assistant at ROSE Lab. <br>
<!-- 2022-04-11: Prof. Miao Fang had accepted me as an undergraduate research assistant at NEU-NLP Lab. <br> -->



# üí° Research Interest
- **Multimodal AI**: My current research goal is to **integrate** multimodal information to improve the performance of large language models, at the same time, I am also seek for applications of multimodal models in **Geolocation** and **Embodied AI** domains.
- **Trustworthy AI**: I am also highly interested and experienced in exploring the *Jailbreak* and *attack* issues of Multimodal Language Models, particularly in the **Vision and Audio** modalities.

# üìñ Educations
  <!-- * 2021.09->2023.06 : B.E.,<img src='files/NEU.png' style='width: 1.2em;'> Northeastern University(NEU) -->
  * 2021.06->2025.05: B.E.,<img src='files/UTS.png' style='width: 1.2em;'><a href="https://www.uts.edu.au/">University of Technology Sydney(UTS)</a>  QS Ranking: 88, U.S. News Ranking: 85 **GPA: 3.93/4.00**
  * 2025.08->2029.05 (Expected) :PhD, <img src='files/mbzuai.png' style='width: 1.2em;'><a href="https://www.mbzuai.ac.ae/">Mohamed Bin Zayed University of Artificial Intelligence(MBZUAI)</a>




# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><img src='files/AnomalyGen.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

 [**Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies**](https://arxiv.org/abs/2411.00781) ![Static Badge](https://img.shields.io/badge/NAACL'2025-red)


**Zirui Song\***, Guangxian Ouyang\*, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Yujie Fu, Zeyu Zhang, Shiyu Jiang, Miao Fang, Ling Chen, Xiuying Chen
(\*: first co-authors)


*"AnomalyGen : A framework of anomaly vitrual scene data generation without human annotation, to enhance the robustness of robots."*
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/BenchLMM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


 [**BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models**](https://link.springer.com/chapter/10.1007/978-3-031-72973-7_20)![Static Badge](https://img.shields.io/badge/ECCV'2024-red)

Rizhao Cai\*, **Zirui Song\***, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, Chenyu Yi & Alex Kot
(\*: first co-authors)

*"BenchLMM: A novel, comprehensive benchmark, specifically designed to investigate the cross-style capability of Large Multimodal Models (LMMs)."*

[**Toolkit & Code**](https://github.com/AIFEG/BenchLMM) [![](https://img.shields.io/github/stars/AIFEG/BenchLMM)](https://github.com/AIFEG/BenchLMM)
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='files/MedINST.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**MedINST: Meta Dataset of Biomedical Instructions**](https://arxiv.org/abs/2410.13458) ![Static Badge](https://img.shields.io/badge/EMNLP'2024-red)


  Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, **Zirui Song**, Ling Chen, Mykola Pechenizkiy, Qingyu Chen

*"MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples."* 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/scholarQA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Unveiling the power of language models in chemical research question answering**](https://www.nature.com/articles/s42004-024-01394-x) ![Static Badge](https://img.shields.io/badge/Communications_Chemistry-red)

  
Xiuying Chen, Tairan Wang, Taicheng Guo, Kehan Guo, Juexiao Zhou, Haoyang Li, **Zirui Song**, Xin Gao & Xiangliang Zhang 

*"ScholarChemQA, a large-scale QA dataset constructed from chemical papers."*
[**Code**](https://github.com/iriscxy/chemmatch)
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><img src='files/geolocation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework**](https://arxiv.org/abs/2502.13759) ![Static Badge](https://img.shields.io/badge/SIGIR2025-submission-green)


 **Zirui Song\***, Jingpu Yang\*, Yuan Huang\*, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, Xiuying Chen
(\*: first co-authors)

*"We introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe"* 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/InjectionSurvey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey**](https://arxiv.org/abs/2502.10708) ![Static Badge](https://img.shields.io/badge/IJCAI2025-submission-green)


 **Zirui Song\***, Bin Yan\*, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen
(\*: first co-authors)

*"LLMs general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization."* 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='files/FUSE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution**](https://arxiv.org/abs/2410.19064) ![Static Badge](https://img.shields.io/badge/ARR2025-Submission-green)


 Yuhan Liu,**Zirui Song**, Xiaoqing Zhang, Xiuying Chen, Rui Yan

*"We take the first step toward simulating and revealing this evolution, proposing a Fake News evolUtion Simulation framEwork (FUSE) based on LLMs"* 

</div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><img src='files/PedDet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection**](https://arxiv.org/abs/2502.13759) ![Static Badge](https://img.shields.io/badge/IJCAI2025-submission-green)


 Rui Zhao\*, Zeyu Zhang\*, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, **Zirui Song**, Xiuying Chen, Yang Zhao
(\*: first co-authors)

*"PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection"* 

</div>
</div>













<div class='paper-box'><div class='paper-box-image'><div><img src='files/MobileAgent.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Foundations and recent trends in multimodal mobile agents: A survey**](https://arxiv.org/abs/2411.02006) ![Static Badge](https://img.shields.io/badge/ARR2025-submission-green)


 Biao Wu\*, Yanda Li\*, Meng Fang, **Zirui Song**, Zhiwei Zhang, Yunchao Wei, Ling Chen
(\*: first co-authors)

*"Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction."* 

Note: The first author Biao Wu canceled my co-authorship during the second submission to ARR2025. And I have not received any explanation from him. But I think I still have credit for this work, and disagree with his decision.

</div>
</div>

















# üíº Experiences
- \[2024.09 - now\] <img src='files/mbzuai.png' style='width: 1.2em;'> MBZUAI (Supervisor: [Prof.Xiuying Chen](https://mbzuai.ac.ae/study/faculty/xiuying-chen/),topic:Trustworthy MLLMs)
- \[2023.10 - 2025.02\] <img src='files/UTS.png' style='width: 1.2em;'> University of Technology Sydney, Research Intern (Supervisor: [Prof.Ling Chen](https://profiles.uts.edu.au/Ling.Chen) and [Prof.Meng Fang](https://mengfn.github.io/),topic: Multimodal Agents)
- \[2023.03 - 2024.01\] <img src='files/ntu.png' style='width: 2.8em;'> Nanyang Technological University, Research Intern (Supervisor: [Prof.Dayan Guan](https://dayan-guan.github.io/),topic: Multimodal LLMs)


# üèÜ Honors and Awards
- ü•à **Silver Medal**, Kaggle - LLM Science Exam [51/2664], 2024
- ü•á **School Second Class Scholarship**Ôºå2022


# üìö Resources

### Blogs

- \[05/24\] \[Chinese\] National Undergraduate Innovation Project Documentation. \[[Link](files/grammargpt-rp.pdf)\] 
- \[03/24\] \[Chinese\] Negative Transfer. \[[Link](https://k034sybliz3.feishu.cn/wiki/GX7Vw4IfBiYq6okUDf7cAGCJnHh)\] 
- \[03/24\] \[Chinese\] Mixture of Experts Explained. \[[Link](https://k034sybliz3.feishu.cn/wiki/MjBFwFm9giBTg3kQ9v6cJ7uQnFb)\] 
- \[01/24\] \[Chinese\] EMNLP2020 Tutorial Notes (Topic: Explainable AI). \[[Link](https://k034sybliz3.feishu.cn/wiki/Mo2xwR6B4iDV7nk4CZ5clwymnze)\] 





# üìú References

You can find my [full CV](files/CV_Zirui.pdf) here (Latest update: **Oct 14th, 2024**).


<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=a&t=tt&d=HluXDGOSdszkRFTgzVSVKaRQAYXHoBEke-V3oK7ljWk'></script>