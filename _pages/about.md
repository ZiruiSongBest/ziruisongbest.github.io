---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

[//]: # ()
[//]: # ()
Hi üëã, I'm Zirui Song(/ÀàziÀêÀåruÀê.i/ /s…îÀê≈ã/), a forth year Undergraduate at University of Technology Sydeny(UTS) majoring in Software Engineering. Also I am an incoming PhD student(starting from August 2025) supervised by <a href="https://scholar.google.com.au/citations?hl=en&user=COUnAF4AAAAJ&view_op=list_works&sortby=pubdate">Prof. Xiuying Chen</a> in <a href="https://www.mbzuai.ac.ae">MBZUAI</a> NLP department . I have become a member of <a href="https://utsnlp.github.io/">UTS-NLP</a> since Oct 2023 where I am fortuante to be advised by  <a href="https://profiles.uts.edu.au/Ling.Chen">Prof. Ling Chen</a>, and be mentored by <a href="https://mengfn.github.io/">Prof. Meng Fang</a>.
I am deeply appreciative of my mentor, <a href="https://dayan-guan.github.io/">Prof. Dayan Guan</a>, who guided me into scientific research.




 My supervisor, Prof. Xiuying Chen is looking for potential PhD students(with 3.5w RMB/month), Master Students(with 3.1w RMB/month) and Interns(1.6w RMB/month for undergraduates and masters. 2w RMB/month for PhD students). All money are tax-free. Feel free to contact [Prof. Chen](xiuying.cheng@mbzuai.ac.ae)  or [me](ziruisong2003@gmail.com) for more information.

# üíª News
 <p style="color: red; font-weight: bold;">
2025-05-16: <a href="">One paper</a>  was accepted by ACL 2025.</p>
<p style="color: red; font-weight: bold;">
 2025-04-20: <a href="">One paper</a>  was accepted by Nature Computational Science.</p>
<p style="color: purple; font-weight: bold;">
2025-03-01: ‚ÄúI was admitted to the PhD program in the NLP department at <a href=""> MBZUAI</a>, where I will commence my PhD studies in August 2025 here.‚Äù</p>
<p style="color: red; font-weight: bold;">
2025-01-23: <a href="">One paper</a>  was accepted by NAACL 2025. </p>
<p style="color: red; font-weight: bold;">
2025-01-02: <a href="">One paper</a>  was accepted by Communications Chemistry. </p>
<p>2024-09-25: First day as a visiting student at MBZUAI under the supervision of Prof. Xiuying Chen. <br></p>
<p style="color: red; font-weight: bold;">
2024-09-20: <a href="">One paper</a>  was accepted by EMNLP 2024. </p>
<p style="color: red; font-weight: bold;">
2024-07-01: <a href="">One paper</a>  was accepted by ECCV 2024. </p>

<details>
    <summary style="font-weight: bold; cursor: pointer; margin-bottom: 10px;"> Click to expand </summary>
    <div>

        <p>2023-11-29: Prof. Ling Chen had accepted me as an undergraduate research assistant at Australia Artificial Intelligence Institute(AAII). <br></p>
        <p>2023-07-01: I am honored to be selected as an international exchange student majoring in Softawre Engineering at UTS. <br></p>
        <p>2023-05-18: Prof. Dayan Guan had accepted me as a remote undergraduate research assistant at ROSE Lab. <br></p>
    </div>
</details>

# Academic Service

- **Reviewer**: NeurIPS 2025, ACM MM 2025, ACL 2025, NAACL 2025, IJCAI 2025, EMNLP 2024, 
# üí° Research Interest
- **Multimodal AI**: My current research goal is to **integrate** multimodal information to improve the performance of large language models, at the same time, I am also seek for applications of multimodal models in **Geolocation** and **Embodied AI** domains.
- **Trustworthy AI**: I am also highly interested and experienced in exploring the *Jailbreak* and *attack* issues of Multimodal Language Models, particularly in the **Vision and Audio** modalities.

# üìñ Educations
  <!-- * 2021.09->2023.06 : B.E.,<img src='files/NEU.png' style='width: 1.2em;'> Northeastern University(NEU) -->
  * 2021.06->2025.05: B.E.,<img src='files/UTS.png' style='width: 1.2em;'><a href="https://www.uts.edu.au/">University of Technology Sydney(UTS)</a>  QS Ranking: 88, U.S. News Ranking: 85. **GPA: 3.93/4.00**
  * 2025.08->2029.05 (Expected) :PhD, <img src='files/mbzuai.png' style='width: 1.2em;'><a href="https://www.mbzuai.ac.ae/">Mohamed Bin Zayed University of Artificial Intelligence(MBZUAI)</a>




# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><img src='files/AnomalyGen.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

 [**Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies**](https://arxiv.org/abs/2411.00781) ![Static Badge](https://img.shields.io/badge/NAACL'2025-red)


**Zirui Song\***, Guangxian Ouyang\*, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Yujie Fu, Zeyu Zhang, Shiyu Jiang, Miao Fang, Ling Chen, Xiuying Chen
(\*: first co-authors)


*"AnomalyGen : A framework of anomaly vitrual scene data generation without human annotation, to enhance the robustness of robots."*
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/BenchLMM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


 [**BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models**](https://link.springer.com/chapter/10.1007/978-3-031-72973-7_20)![Static Badge](https://img.shields.io/badge/ECCV'2024-red)

Rizhao Cai\*, **Zirui Song\***, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, Chenyu Yi & Alex Kot
(\*: first co-authors)

*"BenchLMM: A novel, comprehensive benchmark, specifically designed to investigate the cross-style capability of Large Multimodal Models (LMMs)."*

[**Toolkit & Code**](https://github.com/AIFEG/BenchLMM) [![](https://img.shields.io/github/stars/AIFEG/BenchLMM)](https://github.com/AIFEG/BenchLMM)
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='files/MedINST.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**MedINST: Meta Dataset of Biomedical Instructions**](https://arxiv.org/abs/2410.13458) ![Static Badge](https://img.shields.io/badge/EMNLP'2024-red)


  Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, **Zirui Song**, Ling Chen, Mykola Pechenizkiy, Qingyu Chen

*"MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples."* 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/scholarQA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Unveiling the power of language models in chemical research question answering**](https://www.nature.com/articles/s42004-024-01394-x) ![Static Badge](https://img.shields.io/badge/Communications_Chemistry-red)

  
Xiuying Chen, Tairan Wang, Taicheng Guo, Kehan Guo, Juexiao Zhou, Haoyang Li, **Zirui Song**, Xin Gao & Xiangliang Zhang 

*"ScholarChemQA, a large-scale QA dataset constructed from chemical papers."*
[**Code**](https://github.com/iriscxy/chemmatch)
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><img src='files/geolocation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework**](https://arxiv.org/abs/2502.13759) ![Static Badge](https://img.shields.io/badge/SIGIR2025-submission-green)


 **Zirui Song\***, Jingpu Yang\*, Yuan Huang\*, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, Xiuying Chen
(\*: first co-authors)

*"We introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe"* 

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='files/InjectionSurvey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey**](https://arxiv.org/abs/2502.10708) ![Static Badge](https://img.shields.io/badge/IJCAI2025-submission-green)


 **Zirui Song\***, Bin Yan\*, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen
(\*: first co-authors)

*"LLMs general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization."* 

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='files/FUSE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution**](https://arxiv.org/abs/2410.19064) ![Static Badge](https://img.shields.io/badge/ARR2025-Submission-green)


 Yuhan Liu,**Zirui Song**, Xiaoqing Zhang, Xiuying Chen, Rui Yan

*"We take the first step toward simulating and revealing this evolution, proposing a Fake News evolUtion Simulation framEwork (FUSE) based on LLMs"* 

</div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><img src='files/PedDet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection**](https://arxiv.org/abs/2502.13759) ![Static Badge](https://img.shields.io/badge/IJCAI2025-submission-green)


 Rui Zhao\*, Zeyu Zhang\*, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, **Zirui Song**, Xiuying Chen, Yang Zhao
(\*: first co-authors)

*"PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection"* 

</div>
</div>













<div class='paper-box'><div class='paper-box-image'><div><img src='files/MobileAgent.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

  [**Foundations and recent trends in multimodal mobile agents: A survey**](https://arxiv.org/abs/2411.02006) ![Static Badge](https://img.shields.io/badge/ARR2025-submission-green)


 Biao Wu\*, Yanda Li\*, Meng Fang, **Zirui Song**, Zhiwei Zhang, Yunchao Wei, Ling Chen
(\*: first co-authors)

*"Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction."* 

Note: The first author Biao Wu canceled my co-authorship during the second submission to ARR2025. And I have not received any explanation from him. But I think I still have credit for this work, and disagree with his decision.

</div>
</div>

















# üíº Experiences
- \[2024.09 - now\] <img src='files/mbzuai.png' style='width: 1.2em;'> MBZUAI (Supervisor: [Prof.Xiuying Chen](https://mbzuai.ac.ae/study/faculty/xiuying-chen/),topic:Trustworthy MLLMs)
- \[2023.10 - 2025.02\] <img src='files/UTS.png' style='width: 1.2em;'> University of Technology Sydney, Research Intern (Supervisor: [Prof.Ling Chen](https://profiles.uts.edu.au/Ling.Chen) and [Prof.Meng Fang](https://mengfn.github.io/),topic: Multimodal Agents)
- \[2023.03 - 2024.01\] <img src='files/ntu.png' style='width: 2.8em;'> Nanyang Technological University, Research Intern (Supervisor: [Prof.Dayan Guan](https://dayan-guan.github.io/),topic: Multimodal LLMs)


# üèÜ Honors and Awards
- ü•à **Silver Medal**, Kaggle - LLM Science Exam [51/2664], 2024
- ü•á **School Second Class Scholarship**Ôºå2022


# üìö Resources

### Blogs

- \[05/24\] \[Chinese\] National Undergraduate Innovation Project Documentation. \[[Link](files/grammargpt-rp.pdf)\] 
- \[03/24\] \[Chinese\] Negative Transfer. \[[Link](https://k034sybliz3.feishu.cn/wiki/GX7Vw4IfBiYq6okUDf7cAGCJnHh)\] 
- \[03/24\] \[Chinese\] Mixture of Experts Explained. \[[Link](https://k034sybliz3.feishu.cn/wiki/MjBFwFm9giBTg3kQ9v6cJ7uQnFb)\] 
- \[01/24\] \[Chinese\] EMNLP2020 Tutorial Notes (Topic: Explainable AI). \[[Link](https://k034sybliz3.feishu.cn/wiki/Mo2xwR6B4iDV7nk4CZ5clwymnze)\] 



<!-- # üìù Reflections
During my recent internship at an AI lab, I collaborated with my colleagues, Yuxuan Ji and Yaohang Li, on developing a prototype NLP system aimed at automatically summarizing scientific articles. Our goal was to improve research efficiency by parsing vast amounts of literature and generating concise summaries for quick reference. I was responsible for designing and implementing key modules in Python, while Yuxuan and Yaohang handled data preprocessing and model fine-tuning. We held weekly milestone meetings to assess our progress and address obstacles in data collection, model selection, and system deployment.

At first, I was both excited and a bit overwhelmed‚Äîthis was the first time I applied my software engineering background in a project heavily focused on NLP. The idea of contributing to something that could genuinely benefit researchers worldwide thrilled me, but it also made me anxious about whether my technical decisions would meet the team‚Äôs high standards. Over time, my nervousness subsided, replaced by growing confidence and enthusiasm for our collaborative achievements.

Before this experience, I believed that a strong coding background was enough to tackle most software-based research challenges. However, working on an NLP project shifted my perspective. I came to understand that success in research depends not only on coding skills but also on how well we communicate domain-specific nuances‚Äîparticularly in NLP, where linguistics, data science, and software engineering intersect. I realized that effective teamwork and open dialogue are just as critical as technical expertise.

One major challenge was handling large-scale text datasets. Our team initially underestimated the complexity of preprocessing tens of thousands of academic papers. Another difficulty lay in bridging the knowledge gap: I was more familiar with software architecture, while Yuxuan and Yaohang had deeper NLP expertise. We also needed to clarify jargon and reevaluate certain assumptions so that we could converge on workable solutions.

This experience underscored how integral communication is to interdisciplinary work. By sharing simplified diagrams of model workflows, we saved time explaining algorithmic details. Meanwhile, consistent feedback loops‚Äîboth online and in person‚Äîensured our design decisions were continuously refined. Ultimately, collaboration turned challenges into opportunities for learning, allowing each of us to contribute our strengths.

I learned the importance of balancing coding proficiency with broader research awareness. Going forward, I plan to deepen my NLP knowledge by studying cutting-edge language models and to sharpen my communication skills‚Äîthrough regular presentations and technical writing‚Äîto better convey complex ideas. Moreover, I will proactively set clear objectives for each team discussion, ensuring that we remain focused and efficient. By blending strong technical foundations, open-minded collaboration, and structured communication, I believe I can make more impactful contributions to NLP research in the future.
 -->



# üìú References

You can find my [full CV](https://github.com/ZiruiSongBest/ziruisongbest.github.io/blob/main/files/CV_zirui.pdf) here (Latest update: **Oct 14th, 2024**).


<script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=a&t=tt&d=HluXDGOSdszkRFTgzVSVKaRQAYXHoBEke-V3oK7ljWk'></script>