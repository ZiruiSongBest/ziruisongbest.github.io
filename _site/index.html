<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Zirui Song's Personal Website - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Zirui Song's Personal Website">
<meta property="og:title" content="Zirui Song's Personal Website">


  <link rel="canonical" href="https://github.com/pages/ZiruiSongBest/ziruisongbest.github.io/">
  <meta property="og:url" content="https://github.com/pages/ZiruiSongBest/ziruisongbest.github.io/">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/profile.jpg" class="author__avatar" alt="Zirui Song">
  </div>

  <div class="author__content">
    <h3 class="author__name">Zirui Song</h3>
    <p class="author__bio">University of Technology Sydney</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;"></div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Abu Dhabi, UAE</li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> MBZUAI</li>
      
      
      
        <li><a href="mailto:ziruisong2003@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/ziruibest"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=fmKRIPUAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:ziruisong2003@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/ziruibest"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=fmKRIPUAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p>Hi üëã, I‚Äôm Zirui Song(/ÀàziÀêÀåruÀê.i/ /s…îÀê≈ã/), a forth year Undergraduate at University of Technology Sydeny(UTS) majoring in Software Engineering. Also I am a visiting student supervised by <a href="https://scholar.google.com.au/citations?hl=en&amp;user=COUnAF4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Prof. Xiuying Chen</a> in <a href="https://www.mbzuai.ac.ae">MBZUAI</a> NLP department . I have become a member of <a href="https://utsnlp.github.io/">UTS-NLP</a> since Oct 2023 where I am fortuante to be advised by  <a href="https://profiles.uts.edu.au/Ling.Chen">Prof. Ling Chen</a>, and be mentored by <a href="https://mengf1.github.io/">Prof. Meng Fang</a>.
I am deeply appreciative of my mentor, <a href="https://dayan-guan.github.io/">Prof. Dayan Guan</a>, who guided me into scientific research.</p>

<blockquote>
  <p>I am very excited to discuss potential collaborations, please feel free to <a href="ziruisong2003@gmail.com">contact me</a>.</p>
</blockquote>

<h1 id="-news">üíª News</h1>
<p style="color: red; font-weight: bold;">
2025-01-23: <a href="">One paper</a>  was accepted by NAACL 2025. </p>

<p style="color: red; font-weight: bold;">
2025-01-02: <a href="">One paper</a>  was accepted by Communications Chemistry. </p>

<p>2024-09-25: First day as a visiting student at MBZUAI under the supervision of Prof. Xiuying Chen. <br /></p>

<p style="color: red; font-weight: bold;">
2024-09-20: <a href="">One paper</a>  was accepted by EMNLP 2024. </p>
<p style="color: red; font-weight: bold;">
2024-07-01: <a href="">One paper</a>  was accepted by ECCV 2024. </p>

<p>2024-02-29: Prof. Ling Chen had accepted me as an undergraduate research assistant at Australia Artificial Intelligence Institute(AAII). <br />
2023-07-01: I am honored to be selected as an international exchange student majoring in Softawre Engineering at UTS. <br />
2023-05-18: Prof. Dayan Guan had accepted me as a remote undergraduate research assistant at ROSE Lab. <br />
2022-04-11: Prof. Miao Fang had accepted me as an undergraduate research assistant at NEU-NLP Lab. <br /></p>

<h1 id="-research-interest">üí° Research Interest</h1>
<ul>
  <li><strong>Multimodal AI</strong>: My current research goal is to <strong>integrate</strong> multimodal information to improve the performance of large language models, at the same time, I am also seek for applications of multimodal models in <strong>Geolocation</strong> and <strong>Embodied AI</strong> domains.</li>
  <li><strong>Trustworthy AI</strong>: I am also highly interested and experienced in exploring the <em>Jailbreak</em> and <em>attack</em> issues of Multimodal Language Models, particularly in the <strong>Vision and Audio</strong> modalities.</li>
</ul>

<h1 id="-educations">üìñ Educations</h1>
<ul>
  <li>2021.09-&gt;2023.06 : B.E.,<img src="files/NEU.png" style="width: 1.2em;" /> Northeastern University(NEU)</li>
  <li>2023.06-&gt;2025.07 (Expected) : B.E.,<img src="files/UTS.png" style="width: 1.2em;" /> University of Technology Sydney(UTS)</li>
</ul>

<h3 id="proficiencies">Proficiencies</h3>

<ul>
  <li><strong>WAM:87.6/100</strong> (First Class Honors)</li>
  <li><strong>WES: 3.93/4.00</strong></li>
</ul>

<h3 id="skills">Skills</h3>
<ul>
  <li>I have the necessary theoretical foundation and skills in AI/NLP research, including proficiency in deep learning frameworks (<strong>PyTorch</strong>, <strong>TensorFlow</strong>), training and evaluation techniques, and large-scale data management.</li>
  <li>Also I have some experience in finetuning LLMs(<strong>LLama</strong>) and LVMs(<strong>LLaVA, Qwen-VL</strong>).</li>
</ul>

<h1 id="-publications">üìù Publications</h1>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/AnomalyGen.jpg" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2411.00781"><strong>Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies</strong></a> <img src="https://img.shields.io/badge/NAACL'2025-red" alt="Static Badge" /></p>

    <p><strong>Zirui Song*</strong>, Guangxian Ouyang*, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Yujie Fu, Zeyu Zhang, Shiyu Jiang, Miao Fang, Ling Chen, Xiuying Chen
(*: first co-authors)</p>

    <p><em>‚ÄúAnomalyGen : A framework of anomaly vitrual scene data generation without human annotation, to enhance the robustness of robots.‚Äù</em></p>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/BenchLMM.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-72973-7_20"><strong>BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</strong></a><img src="https://img.shields.io/badge/ECCV'2024-red" alt="Static Badge" /></p>

    <p>Rizhao Cai*, <strong>Zirui Song*</strong>, Dayan Guan, Zhenhao Chen, Yaohang Li, Xing Luo, Chenyu Yi &amp; Alex Kot
(*: first co-authors)</p>

    <p><em>‚ÄúBenchLMM: A novel, comprehensive benchmark, specifically designed to investigate the cross-style capability of Large Multimodal Models (LMMs).‚Äù</em></p>

    <p><a href="https://github.com/AIFEG/BenchLMM"><strong>Toolkit &amp; Code</strong></a> <a href="https://github.com/AIFEG/BenchLMM"><img src="https://img.shields.io/github/stars/AIFEG/BenchLMM" alt="" /></a></p>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/MedINST.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2410.13458"><strong>MedINST: Meta Dataset of Biomedical Instructions</strong></a> <img src="https://img.shields.io/badge/EMNLP'2024-red" alt="Static Badge" /></p>

    <p>Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, <strong>Zirui Song</strong>, Ling Chen, Mykola Pechenizkiy, Qingyu Chen</p>

    <p><em>‚ÄúMedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples.‚Äù</em></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/scholarQA.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://www.nature.com/articles/s42004-024-01394-x"><strong>Unveiling the power of language models in chemical research question answering</strong></a> <img src="https://img.shields.io/badge/Communications_Chemistry-red" alt="Static Badge" /></p>

    <p>Xiuying Chen, Tairan Wang, Taicheng Guo, Kehan Guo, Juexiao Zhou, Haoyang Li, <strong>Zirui Song</strong>, Xin Gao &amp; Xiangliang Zhang</p>

    <p><em>‚ÄúScholarChemQA, a large-scale QA dataset constructed from chemical papers.‚Äù</em>
<a href="https://github.com/iriscxy/chemmatch"><strong>Code</strong></a></p>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/geolocation.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2502.13759"><strong>Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework</strong></a> <img src="https://img.shields.io/badge/SIGIR2025-submission-green" alt="Static Badge" /></p>

    <p><strong>Zirui Song*</strong>, Jingpu Yang*, Yuan Huang*, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, Xiuying Chen
(*: first co-authors)</p>

    <p><em>‚ÄúWe introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe‚Äù</em></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/InjectionSurvey.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2502.10708"><strong>Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey</strong></a> <img src="https://img.shields.io/badge/IJCAI2025-submission-green" alt="Static Badge" /></p>

    <p><strong>Zirui Song*</strong>, Bin Yan*, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen
(*: first co-authors)</p>

    <p><em>‚ÄúLLMs general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization.‚Äù</em></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/FUSE.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2410.19064"><strong>From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution</strong></a> <img src="https://img.shields.io/badge/ARR2025-Submission-green" alt="Static Badge" /></p>

    <p>Yuhan Liu,<strong>Zirui Song</strong>, Xiaoqing Zhang, Xiuying Chen, Rui Yan</p>

    <p><em>‚ÄúWe take the first step toward simulating and revealing this evolution, proposing a Fake News evolUtion Simulation framEwork (FUSE) based on LLMs‚Äù</em></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/PedDet.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2502.13759"><strong>PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection</strong></a> <img src="https://img.shields.io/badge/IJCAI2025-submission-green" alt="Static Badge" /></p>

    <p>Rui Zhao*, Zeyu Zhang*, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, <strong>Zirui Song</strong>, Xiuying Chen, Yang Zhao
(*: first co-authors)</p>

    <p><em>‚ÄúPedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection‚Äù</em></p>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="files/MobileAgent.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2411.02006"><strong>Foundations and recent trends in multimodal mobile agents: A survey</strong></a> <img src="https://img.shields.io/badge/ARR2025-submission-green" alt="Static Badge" /></p>

    <p>Biao Wu*, Yanda Li*, Meng Fang, <strong>Zirui Song</strong>, Zhiwei Zhang, Yunchao Wei, Ling Chen
(*: first co-authors)</p>

    <p><em>‚ÄúMobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction.‚Äù</em></p>

    <p>Note: The first author Biao Wu canceled my co-authorship during the second submission to ARR2025. And I have not received any explanation from him. But I think I still have credit for this work, and disagree with his decision.</p>

  </div>
</div>

<h1 id="-experiences">üíº Experiences</h1>
<ul>
  <li>[2024.09 - now] <img src="files/mbzuai.png" style="width: 1.2em;" /> MBZUAI (Supervisor: <a href="https://mbzuai.ac.ae/study/faculty/xiuying-chen/">Prof.Xiuying Chen</a>,topic:Trustworthy MLLMs)</li>
  <li>[2023.10 - 2025.02] <img src="files/UTS.png" style="width: 1.2em;" /> University of Technology Sydney, Research Intern (Supervisor: <a href="https://profiles.uts.edu.au/Ling.Chen">Prof.Ling Chen</a> and <a href="https://mengfn.github.io/">Prof.Meng Fang</a>,topic: Multimodal Agents)</li>
  <li>[2023.03 - 2024.01] <img src="files/NTU.png" style="width: 2.8em;" /> Nanyang Technological University, Research Intern (Supervisor: <a href="https://dayan-guan.github.io/">Prof.Dayan Guan</a>,topic: Multimodal LLMs)</li>
</ul>

<h1 id="-honors-and-awards">üèÜ Honors and Awards</h1>
<ul>
  <li>ü•à <strong>Silver Medal</strong>, Kaggle - LLM Science Exam [51/2664], 2024</li>
  <li>ü•á <strong>School Second Class Scholarship</strong>Ôºå2022</li>
</ul>

<h1 id="-resources">üìö Resources</h1>

<h3 id="blogs">Blogs</h3>

<ul>
  <li>[05/24] [Chinese] National Undergraduate Innovation Project Documentation. [<a href="files/grammargpt-rp.pdf">Link</a>]</li>
  <li>[03/24] [Chinese] Negative Transfer. [<a href="https://k034sybliz3.feishu.cn/wiki/GX7Vw4IfBiYq6okUDf7cAGCJnHh">Link</a>]</li>
  <li>[03/24] [Chinese] Mixture of Experts Explained. [<a href="https://k034sybliz3.feishu.cn/wiki/MjBFwFm9giBTg3kQ9v6cJ7uQnFb">Link</a>]</li>
  <li>[01/24] [Chinese] EMNLP2020 Tutorial Notes (Topic: Explainable AI). [<a href="https://k034sybliz3.feishu.cn/wiki/Mo2xwR6B4iDV7nk4CZ5clwymnze">Link</a>]</li>
</ul>

<h1 id="-references">üìú References</h1>

<p>You can find my <a href="files/CV_Zirui.pdf">full CV</a> here (Latest update: <strong>Oct 14th, 2024</strong>).</p>

<p><a href="https://mapmyvisitors.com/web/1bw95" title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=RJ-9BpR3nPPhm7slE3OgXRPbI71Yo8jKNdXiKoeSQUw&amp;cl=ffffff" /></a></p>


          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
