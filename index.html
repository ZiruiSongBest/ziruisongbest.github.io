<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zirui Song</title>
  
  <meta name="author" content="Zirui Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zirui Song</name>
              </p>
              <p>  I am a third year Undergraduate at University of Technology Sydeny(UTS). I am working with NTU <a href="https://www.ntu.edu.sg/rose">ROSE</a> <a href="https://dayan-guan.github.io/">Dayan Guan</a>, University of Sydeny <a href="https://a5507203.github.io/">Yu Yao</a> and Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) <a href="https://zhenhaochenofficial.github.io/" >Zhenhao Chen</a>
		   . Previously, I had wonderful experience working with <a href="https://jsjytx.neuq.edu.cn/info/1041/1634.htm">Miao Fang</a> at <a href="https://www.NEU.edu.cn">NEU</a>. I am seeking for the 2025 fall Ph.D position.
<!-- 		   I am the <strong>awardee</strong> of the <a href="https://cra.org/2023-outstanding-undergraduate-researcher-award-recipients/">2023 CRA Outstanding Undergraduate Researcher Award</a>. -->
              </p>
              <p style="text-align:center">
                <a href="zirui.song@student.uts.edu.au">Email</a> &nbsp/&nbsp
                <!-- <a href="images/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=fmKRIPUAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/stevenS28456474">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ziruisongbest">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/zirui.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zirui.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research interests lie in the area of Large Multimodal Models , Vision COT and Prompt Engineering.
              </p>
            </td>
          </tr>
        </tbody></table>
		
        <table style="width:100%;border:0;border-collapse:collapse;border-spacing:0;margin-right:auto;margin-left:auto;"><tbody>
		<tr style="vertical-align:left;">
			    <td style="padding:30px;width:35%;vertical-align:center">
				  <img src='images/BenchLMM.png' width="250">
			    </td>
			  <td style="padding:30px;width:75%;vertical-align:center">
				<a href="https://arxiv.org/abs/2312.02896">
				<papertitle>BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</papertitle>
				</a>
				<br>
				<a href="https://github.com/RizhaoCai">Rizhao Cai*</a>, 
				<strong >Zirui Song*</strong>
				<a href="https://dayan-guan.github.io/">Dayan Guan‚Ä†</a>, 
				<a href="https://zhenhaochenofficial.github.io/">Zhenhao Chen</a>, 
				<a>Xing Luo</a>, 
				<a href="https://github.com/Newbeeyoung">Chenyu Yi</a>, 
				<a href="https://personal.ntu.edu.sg/eackot/">Alex Kot</a>
				<h5 > *Equal contribution, ‚Ä†Corresponding Author </h5>
				<!-- <strong>Zineng Tang</strong>,
				<a href="https://www.microsoft.com/en-us/research/people/ziyiyang/">Ziyi Yang</a>,
				<a href="https://nlp-yang.github.io/">Yang Liu</a>,
				<a href="https://cs.stanford.edu/people/cgzhu/">Chenguang Zhu</a>,
				<a href="https://www.microsoft.com/en-us/research/people/nzeng/">Michael Zeng</a>,
					<a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> -->
				<br>
				<em>Arxiv</em>, 2023
				<br>
				<a href="https://aifeg.github.io/BenchLMM/">Project Page</a>
				  /
				<a href="https://github.com/AIFEG/BenchLMM">github</a>
				/
				<a href="https://arxiv.org/abs/2312.02896">arXiv</a>
				<p></p>
				<p>
					We propose BenchLMM to investigate the cross-style capability of Large Multimodal Models (LMMs). 
				</p>
			  </td>
			</tr>
					
          	<!-- <tr>
				
			  <tr style="vertical-align:left;">
			    <td style="padding:30px;width:35%;vertical-align:center">
				  <img src='images/udop.png' width="250">
			    </td>
			  <td style="padding:30px;width:75%;vertical-align:center">
				<a href="https://arxiv.org/pdf/2212.02623">
				  <papertitle>Unifying Vision, Text, and Layout for Universal Document Processing</papertitle>
				</a>
				<br>
				<strong>Zineng Tang</strong>,
				<a href="https://www.microsoft.com/en-us/research/people/ziyiyang/">Ziyi Yang</a>,
				<a href="https://www.guoxwang.com/">Guoxin Wang</a>,
				<a href="https://www.microsoft.com/en-us/research/people/yuwfan/">Yuwei Fang</a>,
				<a href="https://nlp-yang.github.io/">Yang Liu</a>,
				<a href="https://cs.stanford.edu/people/cgzhu/">Chenguang Zhu</a>,
				<a href="https://www.microsoft.com/en-us/research/people/nzeng/">Michael Zeng</a>,
				<a href="https://www.microsoft.com/en-us/research/people/chazhang/">Cha Zhang</a>,
					<a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>CVPR</em>, 2023 <span style="color: red">(Highlight; 2.5% acceptance rate)</span>
				<br>
				<a href="https://github.com/microsoft/i-Code/tree/main/i-Code-Doc">github</a>
				/
				<a href="https://arxiv.org/pdf/2212.02623">arXiv</a>
				<p></p>
				<p>
				We built a unified framework for document processing.
				</p>
			  </td>
			</tr>
					
          	<tr>
			  <td style="padding:30px;width:35%;vertical-align:middle;padding-top:0px">
				  <img src='images/tvlt.png' width="250">
			  </td>
			  <td style="padding:30px;width:75%;vertical-align:middle;padding-top:0px">
				<a href="https://arxiv.org/abs/2209.14156">
				  <papertitle>TVLT: Textless Vision-Language Transformer</papertitle>
				</a>
				<br>
				<strong>Zineng Tang*</strong>,
				<a href="https://j-min.io/">Jaemin Cho*</a>,
				<a href="https://easonnie.github.io/">Yixin Nie*</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NeurIPS</em>, 2022 <span style="color: red">(Oral; 1.76% acceptance rate)</span>
				<br>
				<a href="https://github.com/zinengtang/TVLT">github</a>
				/
				<a href="https://arxiv.org/abs/2209.14156">arXiv</a>
				<p></p>
				<p>
				We built a textless vision-language transformer with a minimalist design.
				</p>
			  </td>
			</tr>
		  
			<tr>
			  <td style="padding:30px;width:35%;vertical-align:middle;padding-top:0px">
				  <img src='images/vidlankd.png' width="250">
			  </td>
			  <td style="padding:30px;width:75%;vertical-align:middle;padding-top:0px">
				<a href="https://arxiv.org/pdf/2107.02681">
				  <papertitle>Vidlankd: Improving language understanding via video-distilled knowledge transfer</papertitle>
				</a>
				<br>
				<strong>Zineng Tang</strong>,
				<a href="https://j-min.io/">Jaemin Cho</a>,
				<a href="https://scholar.google.com/citations?user=OV1Y3FUAAAAJ&hl=en">Hao Tan</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NeurIPS</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/VidLanKD">github</a>
				/
				<a href="https://arxiv.org/pdf/2107.02681">arXiv</a>
				<p></p>
				<p>
				We built a teacher-student transformer for visually grounded language learning.
				</p>
			  </td>
			</tr>
			
			<tr>
			  <td style="padding:30px;width:35%;vertical-align:middle;padding-top:0px">
				  <img src='images/decembert.png' width="250">
			  </td>
			  <td style="padding:30px;width:75%;vertical-align:middle;padding-top:0px">
				<a href="https://aclanthology.org/2021.naacl-main.193/">
				  <papertitle>Decembert: Learning from noisy instructional videos via dense captions and entropy minimization</papertitle>
				</a>
				<br>
				<strong>Zineng Tang*</strong>,
				<a href="https://jayleicn.github.io/">Jie Lei*</a>,
			  <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>NAACL</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/DeCEMBERT">github</a>
				/
				<a href="https://aclanthology.org/2021.naacl-main.193/">paper</a>
				<p></p>
				<p>
				We built a video-language transformer that addresses the issues of noisy ASR data by dense captions and entropy minimization.
				</p>
			  </td>
			</tr>		
			<tr>
			  <td style="padding:30px;width:35%;vertical-align:middle;padding-top:0px">
				  <img src='images/flow.png' width="250">
			  </td>
			  <td style="padding:30px;width:75%;vertical-align:middle;padding-top:0px">
				<a href="https://aclanthology.org/2021.acl-long.355/">
				  <papertitle>Continuous language generative flow</papertitle>
				</a>
				<br>
				<strong>Zineng Tang</strong>,
				<a href="https://www.cs.unc.edu/~shiyue/">Shiyue Zhang</a>,
				<a href="https://hyounghk.github.io/">Hyounghun Kim</a>,
			    <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>ACL</em>, 2021
				<br>
				<a href="https://github.com/zinengtang/ContinuousFlowNLG">github</a>
				/
				<a href="https://aclanthology.org/2021.acl-long.355/">paper</a>
				<p></p>
				<p>
				We built a language framework basde on continuous generative flow.
				</p>
			  </td>
			</tr>	
            <tr>
			  <td style="padding:30px;width:35%;vertical-align:middle;padding-top:0px">
				  <img src='images/densecap.png' width="250">
			  </td>
			  <td style="padding:30px;width:75%;vertical-align:middle;padding-top:0px">
				<a href="https://arxiv.org/pdf/2005.06409">
				  <papertitle>Dense-caption matching and frame-selection gating for temporal localization in VideoQA</papertitle>
				</a>
				<br>
				<a href="https://hyounghk.github.io/">Hyounghun Kim</a>,
				<strong>Zineng Tang</strong>,
			    <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>
				<br>
				<em>ACL</em>, 2020
				<br>
				<a href="https://github.com/zinengtang/ContinuousFlowNLG">github</a>
				/
				<a href="https://arxiv.org/pdf/2005.06409">arxiv</a>
				<p></p>
				<p>
				We built a video QA framework basde on various techniques like dense captions.
				</p>
			  </td>
			</tr> -->
        </tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Education</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>  
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/Hengshui_High_School.jpg"></td>
				<td width="75%" valign="center">
				  <p><b>Top 2</b> Senior high school in China, 2018 - 2021 </p>
				</td>
			</tr>	
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/NEU.png"></td>
				<td width="75%" valign="center">
				  <p>Northeastern University, B.S. in Computer Science and Technology, 2021 - 2022 </p>
				  <p>After two years of academic pursuits, I was honored to be selected as an exchange student for the University of Technology Sydney, where I will complete the final two years of my undergraduate studies.</p>
				</td>
			</tr>	
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/University_of_Technology_Sydney_emblem.svg.png"></td>
				<td width="75%" valign="center">
				  <p>University of Technology Sydeny, B.S. in Software Engineering, 2023 - Present  </p>
				</td>
			</tr>	
		</tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Experiences</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>  
			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/NJUsvg.png"></td>
				<td width="75%" valign="center">
				  <p><a href="http://nlp.nju.edu.cn/homepage/">NJU-NLP Lab</a>, Summer Camper </p>
				</td>
			</tr>	

			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle"><img width="100" src="images/ROSE.png"></td>
				<td width="75%" valign="center">
				  <p><a href="https://www.ntu.edu.sg/rose/">Rapid-Rich Object Search Lab (ROSE)</a>, Undergraduate research assistant </p>
				</td>
			</tr>
		</tbody></table>
		
		<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
		  <tr>
			<td>
			  <heading>Awards</heading>
			</td>
		  </tr>
		</tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>
							
			<tr>
				<td width="100%" valign="center">
				  <p>NeurIPS 2022 Scholar Award</p>
				  <p>Awardee, Outstanding Undergraduate Researcher Award 2023. Computing Research Association (CRA)</p>
				</td>
			</tr>
		</tbody></table>	   -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template mainly borrowed from <a href="https://jonbarron.info/">Here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
